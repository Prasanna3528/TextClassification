{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Prasanna3528/TextClassification/blob/main/Code_Assgn2_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BZT_HA1Ur6PL"
      },
      "outputs": [],
      "source": [
        "# Import JSON to handle API credentials\n",
        "import json\n",
        "\n",
        "# Set Kaggle API credentials\n",
        "APICredsKaggle = {\n",
        "    \"username\": \"mandavalakshmip\",\n",
        "    \"key\": \"815aeb0f2e186d9ee02dd4cdd782f769\"\n",
        "}\n",
        "\n",
        "# Save credentials to kaggle.json\n",
        "with open('kaggle.json', 'w') as file:\n",
        "    json.dump(APICredsKaggle, file)\n",
        "\n",
        "# Configure Kaggle API access\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "# Download dataset from Kaggle\n",
        "!kaggle datasets download -d purusinghvi/email-spam-classification-dataset\n",
        "\n",
        "# Unzip dataset to folder\n",
        "!unzip -q email-spam-classification-dataset.zip -d SpamEmailData_Org\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Ey-zWuKr6PM"
      },
      "outputs": [],
      "source": [
        "# Install packages that needed to execute the code\n",
        "!pip install -q transformers datasets evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YrXIrD_rr6PN"
      },
      "outputs": [],
      "source": [
        "# Import the libraries needed to execute the code\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from wordcloud import WordCloud\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, classification_report,ConfusionMatrixDisplay\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import torch\n",
        "from datasets import Dataset\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "import evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xqo58M4Fr6PN"
      },
      "outputs": [],
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Initialize lemmatizer\n",
        "Email_Lem = WordNetLemmatizer()\n",
        "\n",
        "# Load English stopwords\n",
        "Email_StopW = set(stopwords.words('english'))\n",
        "\n",
        "def Clean_the_textData(text):\n",
        "    \"\"\"\n",
        "    Clean and preprocess a given text string.\n",
        "\n",
        "    Steps performed:\n",
        "    1. Convert text to lowercase.\n",
        "    2. Remove all punctuation and numeric characters.\n",
        "    3. Tokenize the cleaned text into words.\n",
        "    4. Remove English stopwords (e.g., 'the', 'is', 'and').\n",
        "    5. Lemmatize each word to its base form.\n",
        "    6. Rejoin the processed words into a single string.\n",
        "\n",
        "    Parameters:\n",
        "    text : Raw input text.\n",
        "\n",
        "    Returns: Cleaned and preprocessed text string.\n",
        "    \"\"\"\n",
        "    tolowercase = text.lower()\n",
        "    toremoveAl = re.sub(r'[^a-z\\s]', '', tolowercase)\n",
        "    Toremovetokens = toremoveAl.split()\n",
        "    toLemmatize = [Email_Lem.lemmatize(word) for word in Toremovetokens if word not in Email_StopW]\n",
        "    return ' '.join(toLemmatize)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fevkFmiFr6PO"
      },
      "outputs": [],
      "source": [
        "# Load dataset from CSV file\n",
        "Email_Spam_Data = pd.read_csv(\"/content/SpamEmailData_Org/combined_data.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p9mWVgRpr6PO"
      },
      "outputs": [],
      "source": [
        "# Display first 5 rows of the dataset\n",
        "print(Email_Spam_Data.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N1hRoKn1r6PO"
      },
      "outputs": [],
      "source": [
        "# Show column data types and non-null counts\n",
        "Email_Spam_Data.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gvv49vj3r6PO"
      },
      "outputs": [],
      "source": [
        "# Show stats for numerical columns\n",
        "Email_Spam_Data.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lcn6Y_O7r6PP"
      },
      "outputs": [],
      "source": [
        "# Print dataset shape to know the how many columns and rows are these.\n",
        "Email_df_rows, Email_df_columns = Email_Spam_Data.shape\n",
        "print(f\"The shape of the original dataset is {Email_df_rows} reviews with {Email_df_columns} columns.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6lQQNOj_r6PP"
      },
      "outputs": [],
      "source": [
        "# Show null value count per column\n",
        "Email_Spam_Data.isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G7fr2Ug0r6PP"
      },
      "outputs": [],
      "source": [
        "# Add a new column with the length of each comment\n",
        "Email_Spam_Data['Email_Len'] = Email_Spam_Data['text'].apply(len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hXsqCNDKr6PP"
      },
      "outputs": [],
      "source": [
        "# Apply preprocessing to text column\n",
        "Email_Spam_Data['Email_Cleaned'] = Email_Spam_Data['text'].apply(Clean_the_textData)\n",
        "\n",
        "# Show the difference between original and cleaned text columns\n",
        "Email_Spam_Data[['text', 'Email_Cleaned']]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cbIVs6IBr6PP"
      },
      "outputs": [],
      "source": [
        "# Check for duplicate rows in the dataset\n",
        "print(f\"Number of duplicate rows in the Email Dataset : {Email_Spam_Data.duplicated().sum()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ue-6a3ppr6PP"
      },
      "outputs": [],
      "source": [
        "# Show column names after preprocessing\n",
        "Email_Spam_Data.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A-WUXsSfr6PQ"
      },
      "outputs": [],
      "source": [
        "# Display first 5 rows of the updated dataset\n",
        "Email_Spam_Data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ex79KGaBr6PQ"
      },
      "outputs": [],
      "source": [
        "# Check and display class label distribution\n",
        "Cnt_Tgt = Email_Spam_Data['label'].value_counts()\n",
        "print(\"Label Distribution:\\n\", Cnt_Tgt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YCE8oU0sr6PQ"
      },
      "outputs": [],
      "source": [
        "# Plot the count of class labels\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.countplot(x='label', data=Email_Spam_Data)\n",
        "plt.title(\"Email Classification Label Count\")\n",
        "plt.xlabel(\"Label\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z_gev_Efr6PQ"
      },
      "outputs": [],
      "source": [
        "# Plot histogram of text lengths\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.histplot(Email_Spam_Data['Email_Len'], bins=30, kde=True)\n",
        "plt.title(\"Distribution of Text Lengths of Email Data\")\n",
        "plt.xlabel(\"Number of Words\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a2WrnwzKr6PQ"
      },
      "outputs": [],
      "source": [
        "# Generate WordCloud for each class label\n",
        "for tgt in sorted(Email_Spam_Data['label'].unique()):\n",
        "    EM_txt = ' '.join(Email_Spam_Data[Email_Spam_Data['label'] == tgt]['Email_Cleaned'])\n",
        "    Gen_Wrdcld = WordCloud(width=800, height=400, background_color='white').generate(EM_txt)\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.imshow(Gen_Wrdcld)\n",
        "    plt.axis('off')\n",
        "    plt.title(f\"WordCloud for Label {tgt}\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hdw_5-Thr6PQ"
      },
      "outputs": [],
      "source": [
        "# Specify columns for correlation analysis\n",
        "Num_Col_Email = ['label', 'Email_Len']\n",
        "# Compute correlation matrix\n",
        "Cormat_EM = Email_Spam_Data[Num_Col_Email].corr()\n",
        "# Plot heatmap of correlations\n",
        "sns.heatmap(Cormat_EM, annot=True)\n",
        "# Add title to the heatmap\n",
        "plt.title('Correlation Heatmap of Label and Text Length')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hk_n_V71r6PQ"
      },
      "outputs": [],
      "source": [
        "# Split dataset into training and testing sets with stratified labels\n",
        "Email_Train_df, Email_Test_df = train_test_split(\n",
        "    Email_Spam_Data, test_size=0.2, random_state=2025, stratify=Email_Spam_Data['label']\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Abq07LoKr6PQ"
      },
      "outputs": [],
      "source": [
        "# Convert train and test DataFrames to HuggingFace Dataset format\n",
        "Em_Train_DS = Dataset.from_pandas(Email_Train_df)\n",
        "Em_Test_DS = Dataset.from_pandas(Email_Test_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wTTQH79pr6PQ"
      },
      "outputs": [],
      "source": [
        "# Load BERT tokenizer\n",
        "Chosen_model_EM = \"bert-base-uncased\"\n",
        "Em_Bert_Token = AutoTokenizer.from_pretrained(Chosen_model_EM)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T0rY5BFFr6PQ"
      },
      "outputs": [],
      "source": [
        "# Tokenization function for HuggingFace datasets\n",
        "def fun_Token_EM(data):\n",
        "    \"\"\"\n",
        "    Tokenizes input text using the loaded tokenizer.Applies padding and\n",
        "    truncation to ensure uniform input length for transformer models\n",
        "    like BERT or RoBERTa.\n",
        "\n",
        "    Parameters:\n",
        "    data: A batch of examples with a \"text\" field.\n",
        "\n",
        "    Returns: Tokenized output with input IDs, attention masks, etc.\n",
        "    \"\"\"\n",
        "    return Em_Bert_Token(data[\"text\"], padding=\"max_length\", truncation=True)\n",
        "\n",
        "\n",
        "# Apply tokenization to train and test datasets\n",
        "Em_Train_DS = Em_Train_DS.map(fun_Token_EM, batched=True)\n",
        "Em_Test_DS = Em_Test_DS.map(fun_Token_EM, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yRkhOkGor6PR"
      },
      "outputs": [],
      "source": [
        "# Load pre-trained model for sequence classification with 2 output labels\n",
        "Cust_Model_EM = AutoModelForSequenceClassification.from_pretrained(Chosen_model_EM, num_labels=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YKFFXLfIr6PR"
      },
      "outputs": [],
      "source": [
        "# Set training parameters for the model\n",
        "Em_Btch_Size = 64\n",
        "EM_Log_Steps = len(Em_Train_DS) // Em_Btch_Size\n",
        "# Get the model name without path prefix\n",
        "Chosen_model_EM = Chosen_model_EM.split(\"/\")[-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bns0bq68r6PR"
      },
      "outputs": [],
      "source": [
        "# Define training arguments for model fine-tuning\n",
        "EM_Trn_Args = TrainingArguments(\n",
        "    output_dir=f\"{Chosen_model_EM}-finetuned-custom\", overwrite_output_dir=True,\n",
        "    learning_rate=2e-5, weight_decay=0.01,per_device_train_batch_size=Em_Btch_Size,\n",
        "    per_device_eval_batch_size=Em_Btch_Size, push_to_hub=False,\n",
        "    fp16=torch.cuda.is_available(),logging_steps=EM_Log_Steps,report_to=\"none\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hfF2Es1Hr6PR"
      },
      "outputs": [],
      "source": [
        "# Load accuracy metric from HuggingFace evaluate library\n",
        "Acc_Met_EM = evaluate.load(\"accuracy\")\n",
        "\n",
        "# Define function to compute evaluation metrics\n",
        "def Cal_Metrics_EM(eval_pred):\n",
        "    \"\"\"\n",
        "    Computes accuracy metric for model evaluation.\n",
        "\n",
        "    Parameters:\n",
        "    eval_pred : A tuple containing logits of Raw model predictions and\n",
        "    labels of True labels.\n",
        "\n",
        "    Returns: Dictionary containing accuracy score.\n",
        "    \"\"\"\n",
        "    EM_Logits, EM_labels = eval_pred\n",
        "    EM_preds = torch.argmax(torch.tensor(EM_Logits), dim=-1)\n",
        "    return Acc_Met_EM.compute(predictions=EM_preds, references=EM_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "85UgN9Pkr6PR"
      },
      "outputs": [],
      "source": [
        "# Train the model using HuggingFace Trainer API\n",
        "EM_Trainer_det = Trainer(\n",
        "    model=Cust_Model_EM,\n",
        "    args=EM_Trn_Args,\n",
        "    train_dataset=Em_Train_DS,\n",
        "    eval_dataset=Em_Test_DS,\n",
        "    compute_metrics=Cal_Metrics_EM\n",
        ")\n",
        "# Start training\n",
        "EM_Trainer_det.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lrTj2iTRr6PR"
      },
      "outputs": [],
      "source": [
        "# Evaluate the trained model on the test set\n",
        "EM_Model_Eval = EM_Trainer_det.evaluate()\n",
        "# Print evaluation metrics of the model on Email Spam data\n",
        "print(\"Evaluation Results of the Model on Email Spam Data :\")\n",
        "for key, value in EM_Model_Eval.items():\n",
        "    print(f\"{key}: {value:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "14wLR0WNr6PR"
      },
      "outputs": [],
      "source": [
        "# Get predictions from the model on test data\n",
        "EM_Model_Pred = EM_Trainer_det.predict(Em_Test_DS)\n",
        "Org_Labels = EM_Model_Pred.label_ids\n",
        "EM_Pred_Labels = torch.argmax(torch.tensor(EM_Model_Pred.predictions), axis=1).numpy()\n",
        "# Compute and display confusion matrix\n",
        "Eval_EM_ConfMat = confusion_matrix(Org_Labels, EM_Pred_Labels)\n",
        "Disp_EM_Confmat = ConfusionMatrixDisplay(confusion_matrix=Eval_EM_ConfMat)\n",
        "Disp_EM_Confmat.plot(cmap=\"Blues\", values_format=\"d\")\n",
        "plt.title(\"Confusion Matrix for the Email Spam Detection Evaluation\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u4IIWwqSr6PR"
      },
      "outputs": [],
      "source": [
        "# Get raw text from the original test dataset\n",
        "EM_testing_Samples = Em_Test_DS[\"text\"]\n",
        "# Display 5 prediction samples with their true and predicted labels\n",
        "print(\"\\nPredicting the Lables with the Trained Models on Test Data Samples:\\n\")\n",
        "for i in range(5):\n",
        "    print(f\"Text: {EM_testing_Samples[i]}\")\n",
        "    print(f\"Predicted Label : {EM_Pred_Labels[i]}, True Label: {Org_Labels[i]}\")\n",
        "    print(\"-\" * 60)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}